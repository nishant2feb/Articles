---
title: "h2o"
author: "Nishant"
date: "October 4, 2018"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
```

## Describing H2O

There is a lot of buzz for machine learning algorithms as well as requirement for its experts. We all know that there is a significant gap in the skill requirement.
The motive of H2O is to provide a platform which made easy for the non experts to do experiments with machine learning.

H2O's core code is written in java that enables the whole framework for multi-threading.
Although it is written in java but it provide interfaces for R, Python and Scala, thus enabling its use much easy.

In crux we can say that H2O is an open source, in memory, distributed, fast and scalable machine learning and predictive analytics that allows to build machine learning models with an ease. 

## Installation process

### R

If you want to use H2O functionality in R you can simply install package H2O using command `install.packages("h2o")`. 

```{r warning=FALSE}
library(h2o)

h2o.init()

```

Initialising H2O might throw an error in your system in case if you dont have Jdk of 64 bit. If such issue arises please install latest Jdk of 64 bits, it will work smoothly afterwards.


### Python
 
If you are using python the same method is applied in it too, from command line `pip install -U h2o` and h2o will be installed for your python enviornment. The same process will go on for initailising h2o.

![Python Code](Capture2)

The h2o.init() command is pretty smart and does a lot of work. At first it looks for any active h2o instance before starting a new one and starts a new when instance are not present.

It do have arguments which helps to accomodate resources to the h2o instance frequently used are:

* nthreads: By default the value of nthreads will be -1 which means the instance can use all the cores of the CPU, we can set the number of cores utilized by passing the value to the argument.

* max_mem_size: By passing value to this argument you can restrict the maximum memory allocated to the instance. Its is od string type can pass argument as '2g' or '2G' for 2 GBs of memeory, same when you want to allocate in MBs.


Once instance is created you can access the flow by typing http://localhost:54321 in your browser.
Flow is the name of the web interface that is part of h2o which does not require any extra installations which is wriiten in CoffeeScript(a JavaScript like language).
You can use it for doing following things:

* Upload data directly
* View data uploaded by the client
* Create models directly
* View models created by you or your client
* view predictions
* Run preditcions directly

![flow](Capture1)

The interface is quite useful and provide an ease of use to non experts, I would reccommend to try it and perform some experiments of your own.

## AutoML

Now talking about AutoML part of H2O, AutoML helps in automatic training and tuning of many models within a user specified time limit.

The current version of AutoML function can train and cross validate a Random Forest, an Extremely-Randomized Forest, a random grid of Gradient Boosting Machines (GBMs), a random grid of Deep Neural Nets, and then trains a Stacked Ensemble using all of the models.

When we say AutoML it should cater the aspects of data preparation, Model generation and Ensembles and also provide few parameters as possible so that users can perform tasks with much less confusion. H2o AutoML do performs this task in ease and minimal parameter passed by the user.

In both R and Python API, it uses same data related arguments `x`, `y`, `training_frame`, `validation frame` out of which `y` and `training_frame` are required parameter and rest are optional. You can also configure values for `max_runtime_sec` amd `max_models` here `max_runtime_sec` parameter is required and `max_model` is optional if you don't pass any parameter it takes NULL by default.

The `x` parameter is the vector of predictors from `training_frame` if you don't want to use all predictors from frame you passed you can set it by passing it to `x`.

Now lets talk about some optional and miscellaneous parameters, try to tweak the parameters even if you don't know about it, it will lead you to gain knowledge over some advance topics:

* validation_frame: This parameter is used for early stopping of individual models in the automl. It is data frame that you pass for validation of a model or can be a part of training data if not passed by you.

* leaderboard_frame: If passed the models will be scored according to the values instead of using cross validation metrics. Again the values are a part of training data if not passed by you.

* nfolds: K-fold cross validation by default 5, can be used to decrease the model perdormance.

* fold_columns: Specifies the index for cross validation.

* weights_column: If you want to provide weights to specific columns you can use this parameter, assigning weight 0 means you are excluding the column.

* ignored_columns: Only in python, it is converse of `x`.

* stopping_metric: Specifies a metric for early stopping of the grid searches and models default value are logloss for classification and deviation for regression.

* sort_metric: The parameter to sort the learderboard models at the end. This defaults to AUC for binary classification, mean_per_class_error for multinomial classification, and deviance for regression.

The validation_frame and leaderboard_frame depends on the cross validation parameter that is `nfolds`.

The following scenarios can generate in two cases:

when we are using cross validation in the automl:
* Only taining frame is passed - Then data will split into 80-20 of training and validation frame.
* tarining and leaderboard frame is passed - No change in split again 80-20 split of data in training and validation frame.
* When training and validation frame is passed - No split.
* when all three frames are passed - No splits.

When we are not using cross validation which will effect the leaderboard frame a lot(nfolds = 0):
* Only training frame is passed - The data is split into 80/10/10 training, validation and leaderboard.
* training and leaderboard frame is passed - Data split into 80-20 of training and validation frames.
* When training and validation frame is passed - The validation_frame data is split into 50-50 validation and leaderboard.
* when all three frames are passed - No splits.


## Implementation In R and Python

### R 

```{r}

```

### Python

![Python code](Capture3)








